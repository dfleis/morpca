% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/morpca.R
\name{morpca}
\alias{morpca}
\title{Manifold Optimization for Robust PCA}
\usage{
morpca(Y = NULL, r = NULL, gamma = NULL, sparsity = NULL,
  retraction = c("projective", "orthographic"), stepsize = NULL,
  maxiter = 100, stepsout = F, verbose = F)
}
\arguments{
\item{Y}{Input matrix composed of the sum of matrices \eqn{L^*}
(signal) and \eqn{S^*} (noise).}

\item{r}{Rank of the underlying matrix \eqn{L^*} and its estimate.}

\item{gamma}{Value between 0 and 1 corresponding to percentile
for the hard thresholding procedure.}

\item{sparsity}{TO DO...}

\item{retraction}{String specifying which retraction technique
should be applied. Currently implemented are the
\code{"projective"} and \code{"orthographic"}
retractions.}

\item{stepsize}{Positive nonzero number corresponding to the step size
\eqn{\eta} in the gradient descent algorithm.}

\item{maxiter}{Positive nonzero integer specifying the maximum number
of steps to compute for the gradient descent algorithm.}

\item{stepsout}{Boolean value. If \code{stepsout = T} then the function
returns the output low rank matrix estimate and corresponding
gradient for every step in the gradient descent algorithm.}

\item{verbose}{Boolean value. If \code{verbose = T} then the gradient
descent algorithm reports the current step number and
value of the objective function at each iteration.}
}
\value{
Returns an object of class \code{morpca} with elements:

\item{Y}{The original observations used as the input matrix for which we
seek an underlying low rank matrix.}
\item{rank}{Rank of the estimated target underlying matrix.}
\item{gamma}{Thresholding percentile.}
\item{sparsity}{TO DO...}
\item{retraction}{TO DO...}
\item{stepsize}{TO DO...}
\item{maxiter}{TO DO...}
\item{solution}{Estimated underlying low rank matrix. List of matrices if \code{stepsout = T}.}
\item{gradient}{Corresponding gradient matrix of the objective function. List of matrices if \code{stepsout = T}.}
\item{objective}{Corresponding value of the objective function (Frobenius norm of the gradient matrix).}
}
\description{
TO DO...
}
\details{
Implementation of the robust PCA algorithms outlined in
Zhang, T. and Yang, Y. (forthcoming) to recover the underlying low rank
matrix \eqn{L^*} given an input matrix \eqn{Y} assumed to be of the form
\deqn{Y = L^* + S^*,} where \eqn{S^*} is a sparse matrix representing the
noise and \eqn{L^*} representing the signal. This implementation offers
both the projective and orthographic retractions as methods to map from
the input matrix's tangent space back to the manifold of low rank matrices.
Robustness is achieved via a hard thresholding function which sets
entry \eqn{(i,j)} to 0 if it exceeds the \eqn{\gamma}-th percentile of the
\eqn{i}-th row and \eqn{j}-th column (see Zhang, T. and Yang, Y. (forthcoming)
for details).

TO DO... Talk about the objective function and a brief overview of
the gradient descent process.
}
